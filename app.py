# Importa a biblioteca Streamlit, usada para criar a interface web.
import random
import streamlit as st
import streamlit.components.v1 as components
# Importa a biblioteca do Google Generative AI para interagir com o modelo Gemini.
import google.generativeai as genai
# Importa a biblioteca os para interagir com o sistema operacional (usada aqui para ler vari√°veis de ambiente).
import os
# Importa a fun√ß√£o load_dotenv da biblioteca python-dotenv para carregar vari√°veis de ambiente de um arquivo .env.
from dotenv import load_dotenv

# Carrega as vari√°veis de ambiente do arquivo .env.
load_dotenv()

# --- Configura√ß√£o do Modelo Gemini ---

# Configura a API do Gemini com a chave que est√° na vari√°vel de ambiente.
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Define a instru√ß√£o de sistema que guiar√° o comportamento do nosso agente de IA.
# Esta √© a parte mais CR√çTICA para o nosso projeto.
# Ela define a "personalidade" e as regras que o nosso tutor deve seguir.
system_instruction = """
Voc√™ √© um tutor de IA especializado em ajudar estudantes do ensino fundamental e m√©dio. Seu nome √© <nome da ia kk>.
Seu objetivo N√ÉO √© dar as respostas prontas. Seu objetivo √© guiar o aluno a encontrar a resposta por conta pr√≥pria.

Siga estritamente as seguintes regras:
1.  **Apresente-se e Pe√ßa um T√≥pico**: Comece a conversa se apresentando como "<nomeIA>" e pergunte ao aluno qual mat√©ria ou t√≥pico ele gostaria de estudar hoje.
2.  **Crie uma Pergunta**: Assim que o aluno disser o t√≥pico, crie uma pergunta ou um problema desafiador, mas apropriado para o n√≠vel escolar, sobre esse t√≥pico.
3.  **NUNCA D√™ a Resposta Direta**: Em hip√≥tese alguma, forne√ßa a resposta final para a pergunta que voc√™ criou.
4.  **Guie com Pistas e Explica√ß√µes**: Se o aluno n√£o souber a resposta ou errar, quebre o problema em partes menores. Explique os conceitos fundamentais passo a passo. Use analogias simples e exemplos do dia a dia.
5.  **Verifique a Compreens√£o**: A cada pequena explica√ß√£o, fa√ßa uma pergunta para verificar se o aluno est√° entendendo. Use frases como "Fez sentido pra voc√™?", "Consegue pensar em um exemplo disso?", "O que voc√™ acha que vem a seguir?".
6.  **Seja Encorajador e Paciente**: Mantenha um tom positivo e motivador. Elogie o esfor√ßo do aluno. Frases como "√ìtima tentativa!", "Estamos quase l√°!", "Excelente pergunta!" s√£o muito bem-vindas.
7.  **Conduza √† Resposta**: Continue dando pistas e explicando os conceitos at√© que o pr√≥prio aluno consiga formular a resposta correta. Quando ele acertar, parabenize-o e fa√ßa um breve resumo do que foi aprendido.
8.  **Explique a l√≥gica, analogias, etc., de forma que crian√ßas e adolescente entendam de maneira f√°cil.
"""

# Inicializa o modelo generativo do Gemini, especificando o modelo a ser usado ('gemini-1.5-pro-latest').
# Passa a instru√ß√£o de sistema para definir o comportamento do modelo.
# MUDAR VERS√ÉO
model = genai.GenerativeModel(
    model_name="gemini-2.5-fast",
    system_instruction=system_instruction
)

# --- Interface do Streamlit ---

# Define o t√≠tulo da p√°gina da aplica√ß√£o web.
st.title("ü§ñ AprendaAI: Seu Tutor de IA")
# Escreve um subt√≠tulo ou uma descri√ß√£o breve para o usu√°rio.
st.write("Ol√°! Eu sou o <nomeIA>. Estou aqui para te ajudar a aprender, n√£o para te dar as respostas. Vamos estudar juntos?")

# Verifica se o 'chat' (a sess√£o de conversa) j√° foi inicializado no estado da sess√£o do Streamlit.
if "chat" not in st.session_state:
    # Se n√£o foi, inicia uma nova sess√£o de chat com o modelo Gemini.
    st.session_state.chat = model.start_chat(history=[])

# Itera sobre o hist√≥rico de mensagens armazenado no estado da sess√£o.
# O hist√≥rico √© uma lista de dicion√°rios, onde cada dicion√°rio representa uma mensagem.
for message in st.session_state.chat.history:
    # Define o "avatar" a ser exibido ao lado da mensagem, dependendo do papel ('user' ou 'model').
    avatar = "üë§" if message.role == "user" else "ü§ñ"
    # Usa st.chat_message para exibir a mensagem na interface de chat com o avatar apropriado.
    with st.chat_message(message.role, avatar=avatar):
        # Exibe o conte√∫do da mensagem (a parte de texto).
        st.markdown(message.parts[0].text)

#lista de frases para o placeholder
listTextsPlaceholderInputPrompt = ["O que deseja saber? (:", "Qual mat√©ria voc√™ prefere revisar hoje?",
             "O que voc√™ gostaria de estudar agora?", "Qual disciplina est√° com mais vontade de focar hoje?",
             "Que assunto voc√™ quer aprender neste momento?", "Qual mat√©ria voc√™ acha melhor come√ßar hoje?",
             "O que voc√™ gostaria de revisar primeiro?", "Qual disciplina voc√™ est√° a fim de estudar hoje?",
            "Que tema voc√™ prefere aprofundar agora?", "Qual mat√©ria voc√™ sente que precisa praticar hoje?",
            "O que voc√™ quer estudar nesta sess√£o?"
             ]
#gerando √≠ndice aleat√≥rio de 1 a 11
indexTexts = random.randint(0, 10) 

# Cria um campo de entrada de texto na parte inferior da tela, com um placeholder.
# A vari√°vel 'prompt' conter√° o texto que o usu√°rio digitar.
# com placeholder randomizado
if prompt := st.chat_input(listTextsPlaceholderInputPrompt[indexTexts]):
    # Exibe a mensagem do usu√°rio na interface de chat imediatamente.
    with st.chat_message("user", avatar="üë§"):
        # Mostra o prompt que o usu√°rio digitou.
        st.markdown(prompt)

    # Envia a mensagem do usu√°rio para o modelo Gemini e aguarda a resposta.
    # O Streamlit mostrar√° um indicador de "rodando" enquanto espera.
    response = st.session_state.chat.send_message(prompt)

    # Exibe a resposta do modelo (do nosso tutor IA) na interface de chat.
    with st.chat_message("model", avatar="ü§ñ"):
        # Mostra o texto da resposta recebida do Gemini.
        st.markdown(response.text)

#codigo html
components.html(
    """
        
    """,
    height=300
)

#c√≥digo css
st.markdown(
    """
        <style>
            *{
                
            }
        </style>

    """,
    unsafe_allow_html=True
)